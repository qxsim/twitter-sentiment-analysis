{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "910WrU39lHEl"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'tensorflow'\n",
    "import multiprocessing\n",
    "import csv\n",
    "import gensim\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import optuna\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from sklearn import utils\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6C2Dh0vXmorw"
   },
   "outputs": [],
   "source": [
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "cats = ['tag', 'tweet']\n",
    "df = pd.read_csv(\"tweets.csv\", header=None, usecols=[0, 5], names=cats, encoding='latin-1')\n",
    "clean_df = pd.read_csv(\"cleaned_tweets.csv\", encoding='latin-1', names=cats)\n",
    "df['tag'] = df['tag'].map({0: 0, 4: 1})\n",
    "wpt = WordPunctTokenizer()\n",
    "train_corpus = list()\n",
    "cores = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_v2.model'\n",
    "model_name_dmm = 'doc2vecmodel_v2_dmm.model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JrL0QnpRmpoN"
   },
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    mention_remove = r'@[A-Za-z0-9_]+'\n",
    "    link_remove = r'https?://[^ ]+'\n",
    "    www_remove = r'www.[^ ]+'\n",
    "    replace_neg_dict = {\"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "                        \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\", \"won't\": \"will not\",\n",
    "                        \"wouldn't\": \"would not\", \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "                        \"can't\": \"can not\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"mightn't\": \"might not\",\n",
    "                        \"mustn't\": \"must not\"}\n",
    "    neg_replace = re.compile(r'\\b(' + '|'.join(replace_neg_dict.keys()) + r')\\b')\n",
    "\n",
    "    soup = BeautifulSoup(tweet, 'lxml')\n",
    "    nohtml = soup.get_text()\n",
    "\n",
    "    try:\n",
    "        cleaned = nohtml.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        cleaned = nohtml\n",
    "\n",
    "    cleaned = re.sub(mention_remove + '|' + link_remove, '', cleaned)\n",
    "    cleaned = re.sub(www_remove, '', cleaned)\n",
    "    cleaned = cleaned.lower()\n",
    "    cleaned = neg_replace.sub(lambda x: replace_neg_dict[x.group()], cleaned)\n",
    "    cleaned = re.sub(\"[^a-zA-Z]\", \" \", cleaned)\n",
    "\n",
    "    words = []\n",
    "\n",
    "    for x in wpt.tokenize(cleaned):\n",
    "        if len(x) > 1:\n",
    "            words.append(x)\n",
    "\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kHt6HgGpmtSR"
   },
   "outputs": [],
   "source": [
    "def clean_all_tweets():\n",
    "    print(\"Cleaning tweets...\\n\")\n",
    "    clean_tweets = []\n",
    "    for i in range(0, len(df)):\n",
    "        if (i + 1) % 100000 == 0:\n",
    "            print(\"Tweets %d of %d have been cleaned\" % (i + 1, len(df)))\n",
    "        clean_tweets.append(clean_tweet(df.tweet[i]))\n",
    "\n",
    "    cleaned_collection = pd.DataFrame(clean_tweets, columns=['tweet'])\n",
    "    cleaned_collection['tag'] = df.tag\n",
    "    cleaned_collection = cleaned_collection[['tag', 'tweet']]\n",
    "    cleaned_collection.to_csv('cleaned_tweets_wsw_v2.csv', encoding='latin-1', index=False, quoting=csv.QUOTE_ALL,\n",
    "                              header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7xyJMpYamwOm"
   },
   "outputs": [],
   "source": [
    "def load_tweets(tweets):\n",
    "    clean_df.dropna(inplace=True)\n",
    "    clean_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jXhhu_kfKj0I"
   },
   "outputs": [],
   "source": [
    "def create_tweet_samples(tweets, size):\n",
    "    set_size = (size // 2)\n",
    "    print(set_size)\n",
    "    set1 = tweets[tweets.tag == 0].sample(n=(set_size))\n",
    "    set2 = tweets[tweets.tag == 1].sample(n=(set_size))\n",
    "    \n",
    "    all_set = set1.append(set2)\n",
    "    all_set.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    sample_title = 'tweets_sample_' + str(size) + '.csv'\n",
    "    \n",
    "    all_set.to_csv(sample_title, encoding='latin-1', index=False, quoting=csv.QUOTE_ALL, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sMbbEfDQmyT3"
   },
   "outputs": [],
   "source": [
    "def tokenize_tweet(tweet):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(tweet):\n",
    "        for word in nltk.word_tokenize(sent):\n",
    "            if len(word) < 1:\n",
    "                continue\n",
    "            tokens.append(word.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTiHExH6m0Qb"
   },
   "outputs": [],
   "source": [
    "def make_tagged_docs(tweets):\n",
    "    train_corpus.clear()\n",
    "    for i in range(0, len(tweets)):\n",
    "        if (i + 1) % 10000 == 0:\n",
    "            print(\"Tweets %d of %d have been tagged\" % (i + 1, len(df)))\n",
    "        tagged_doc = TaggedDocument(tokenize_tweet(clean_df.tweet[i]), [i])\n",
    "        train_corpus.append(tagged_doc)\n",
    "    print('DOCS ALL TAGGED')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qAJJJrhXm2U3"
   },
   "outputs": [],
   "source": [
    "def model_to_vec(docvecmodel, tweets, dim):\n",
    "    vectors = np.zeros((len(tweets), dim))\n",
    "    n = 0\n",
    "    for m in tweets.index:\n",
    "        vectors[n] = docvecmodel.docvecs[m]\n",
    "        n += 1\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h7nvip7-Kj0c"
   },
   "outputs": [],
   "source": [
    "def model_combine(model1, model2, tweets, dim):\n",
    "    vectors = np.zeros((len(tweets), dim))\n",
    "    n = 0\n",
    "    for m in tweets.index:\n",
    "        vectors[n] = np.append(model1.docvecs[m],model2.docvecs[m])\n",
    "        n += 1\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BaJ9UwwZm5uy"
   },
   "outputs": [],
   "source": [
    "def average_vectors(x_vectors, y_values, array_size):\n",
    "    pos_tweets = np.zeros((sum(y_values.values == 1), array_size))\n",
    "    neg_tweets = np.zeros((sum(y_values.values == 0), array_size))\n",
    "\n",
    "    m = 0\n",
    "    n = 0\n",
    "    o = 0\n",
    "    for i, j in zip(x_vectors, y_values):\n",
    "        if j == 0:\n",
    "            neg_tweets[m] = x_vectors[n]\n",
    "            m += 1\n",
    "            n += 1\n",
    "        else:\n",
    "            pos_tweets[o] = x_vectors[n]\n",
    "            o += 1\n",
    "            n += 1\n",
    "\n",
    "    pos_avg = np.average(pos_tweets, axis=0)\n",
    "    neg_avg = np.average(neg_tweets, axis=0)\n",
    "\n",
    "    return pos_avg, neg_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u5Nu_bGSm7zl"
   },
   "outputs": [],
   "source": [
    "def cosine_sim(pos, neg, tweetvec, tags, array_size):\n",
    "    match_arr = list()\n",
    "    pos = pos.reshape(1, array_size)\n",
    "    neg = neg.reshape(1, array_size)\n",
    "\n",
    "    for i, j in zip(tweetvec, tags):\n",
    "        i = i.reshape(1, array_size)\n",
    "        pos_sim = cosine_similarity(pos, i)[0][0]\n",
    "        neg_sim = cosine_similarity(neg, i)[0][0]\n",
    "        cos_tag = 0\n",
    "        match = False\n",
    "\n",
    "        if pos_sim > neg_sim:\n",
    "            cos_tag = 1\n",
    "        else:\n",
    "            cos_tag = 0\n",
    "        \n",
    "        if j == cos_tag:\n",
    "            match = True\n",
    "            match_arr.append(match)\n",
    "        else:\n",
    "            match = False\n",
    "            match_arr.append(match)\n",
    "\n",
    "    sum_of_matches = sum(match_arr)\n",
    "    accuracy = sum_of_matches/len(tags)\n",
    "\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Y6GRb5XAkKH"
   },
   "outputs": [],
   "source": [
    "def cosine_sim_analyse(tweet, pos, neg, array_size):\n",
    "    pos = pos.reshape(1, array_size)\n",
    "    neg = neg.reshape(1, array_size)\n",
    "\n",
    "    tweet = tweet.reshape(1, array_size)\n",
    "    pos_sim = cosine_similarity(pos, tweet)[0][0]\n",
    "    neg_sim = cosine_similarity(neg, tweet)[0][0]\n",
    "    \n",
    "    if pos_sim > neg_sim:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fn1RFjriKj0t"
   },
   "outputs": [],
   "source": [
    "def train_dbow_d2v(model_name, dim):\n",
    "    model_dbow = Doc2Vec(dm=0, vector_size=dim, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "    model_dbow.build_vocab([x for x in tqdm(train_corpus)])\n",
    "    print('DONE BUILDING DBOW VOCAB')\n",
    "\n",
    "    for epoch in range(30):\n",
    "        model_dbow.train(utils.shuffle([x for x in tqdm(train_corpus)]), total_examples=len(train_corpus), epochs=1)\n",
    "        model_dbow.alpha -= 0.002\n",
    "        model_dbow.min_alpha = model_dbow.alpha\n",
    "\n",
    "    print('TRAINED DBOW DOC2VEC MODEL SUCCESSFULLY')\n",
    "    \n",
    "    model_dbow.save(model_name)\n",
    "    print('SAVED DBOW MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5q4hPF6Kj0x"
   },
   "outputs": [],
   "source": [
    "def train_dmm_d2v(model_name, dim):\n",
    "    model_dmm = Doc2Vec(dm=1, dm_mean=1, vector_size=dim, negative=5, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "\n",
    "    model_dmm.build_vocab([x for x in tqdm(train_corpus)])\n",
    "    print('DONE BUILDING DMM VOCAB')\n",
    "\n",
    "    for epoch in range(30):\n",
    "        model_dmm.train(utils.shuffle([x for x in tqdm(train_corpus)]), total_examples=len(train_corpus), epochs=1)\n",
    "        model_dmm.alpha -= 0.002\n",
    "        model_dmm.min_alpha = model_dmm.alpha\n",
    "\n",
    "    print('TRAINED DMM DOC2VEC MODEL SUCCESSFULLY')\n",
    "    \n",
    "    model_dmm.save(model_name)\n",
    "    print('SAVED DMM MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordvectors(model, tweet, dim):\n",
    "    vector = np.zeros((dim))\n",
    "    vector = vector.reshape(1, dim)\n",
    "    count = 0.0\n",
    "    \n",
    "    for i in tweet.split():\n",
    "        try:\n",
    "            vector += model[i].reshape(1, dim)\n",
    "            count += 1.0\n",
    "        except KeyError:\n",
    "            continue\n",
    "    \n",
    "    if count != 0:\n",
    "        vector = vector / count\n",
    "\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_wordvec(model, dim):\n",
    "    \n",
    "    train_vec = np.concatenate([get_wordvectors(model, x, dim) for x in x_train])\n",
    "    validation_vec = np.concatenate([get_wordvectors(model, x, dim) for x in x_validation])\n",
    "    test_vec = np.concatenate([get_wordvectors(model, x, dim) for x in x_test])\n",
    "    \n",
    "    return train_vec, validation_vec, train_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gq8TkL3696Ru"
   },
   "outputs": [],
   "source": [
    "def model_eval(model_1, dim, model_2=None):\n",
    "\n",
    "    if model_2!=None:\n",
    "        train_vec = model_combine(model_1, model_2, x_train, dim)\n",
    "        val_vec = model_combine(model_1, model_2, x_validation, dim)\n",
    "        test_vec = model_combine(model_1, model_2, x_test, dim)\n",
    "    else:\n",
    "        train_vec = model_to_vec(model_1, x_train, dim)\n",
    "        val_vec = model_to_vec(model_1, x_validation, dim)\n",
    "        test_vec = model_to_vec(model_1, x_test, dim)\n",
    "    \n",
    "    print('MODEL EVALUATION REPORT:')\n",
    "    print('-----------------------------------')\n",
    "   \n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(train_vec, y_train)\n",
    "    lr_acc_score = lr.score(val_vec, y_validation)\n",
    "    lr_acc_score = str(lr_acc_score)\n",
    "    print('Logistic Regression Score: ' + lr_acc_score)\n",
    "    print('     ----------     ')\n",
    "\n",
    "    posavg, negavg = average_vectors(train_vec, y_train, dim)\n",
    "    cos_similarity = cosine_sim(posavg, negavg, val_vec, y_validation, dim)\n",
    "    cs_acc_score = str(cos_similarity)\n",
    "    print('Cosine Similarity Score: ' + cs_acc_score)\n",
    "    print('     ----------     ')\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "    lda.fit(train_vec, y_train)\n",
    "    lda_acc_score = lda.score(val_vec, y_validation)\n",
    "    lda_acc_score = str(lda_acc_score)\n",
    "    print('Linear Discriminant Analysis Score: ' + lda_acc_score)\n",
    "    print('     ----------     ')\n",
    "\n",
    "    return lr, posavg, negavg, lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "po79qfV-aDa6"
   },
   "outputs": [],
   "source": [
    "def get_sentiment(tweet, model_1, dim, lr, posavg, negavg, lda, model_2=None):\n",
    "    cleaned = clean_tweet(tweet)\n",
    "    tokenized = tokenize_tweet(cleaned)\n",
    "    array_size = dim\n",
    "\n",
    "    if model_2!=None:\n",
    "        vector1 = model_1.infer_vector(doc_words=tokenized, epochs=30, alpha=0.065)\n",
    "        vector2 = model_2.infer_vector(doc_words=tokenized, epochs=30, alpha=0.065)\n",
    "        vector = np.append(vector1,vector2)\n",
    "\n",
    "    else:\n",
    "        vector = model_1.infer_vector(doc_words=tokenized, epochs=30, alpha=0.065)\n",
    "\n",
    "\n",
    "    print('SENTIMENT ANALYSIS REPORT:')\n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    lr_score = lr.predict([vector])\n",
    "    lr_score = re.findall(r'\\d+', str(lr_score))\n",
    "    lr_score = lr_score[0]\n",
    "    print('Logistic Regression estimate: ' + lr_score)\n",
    "    print('     ----------     ')\n",
    "\n",
    "    cs_score = cosine_sim_analyse(vector, posavg, negavg, array_size)\n",
    "    cs_score = str(cs_score)\n",
    "    print('Cosine Similarity estimate: ' + cs_score)\n",
    "    print('     ----------     ')\n",
    "    \n",
    "    lda_score = lda.predict([vector])\n",
    "    lda_score = re.findall(r'\\d+', str(lda_score))\n",
    "    lda_score = lda_score[0]\n",
    "    print('Linear Discriminant Analysis estimate: ' + lda_score)\n",
    "    print('     ----------     ')\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lsXqJwNAKj01"
   },
   "outputs": [],
   "source": [
    "clean_all_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4YBzSMOm_Li"
   },
   "outputs": [],
   "source": [
    "load_tweets(clean_df)\n",
    "x = clean_df.tweet\n",
    "y = clean_df.tag\n",
    "x_train, x_remain, y_train, y_remain = train_test_split(x, y, test_size=.02, random_state=2000) # (98:2 TRAIN:REMAIN SPLIT) \n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_remain, y_remain, test_size=.5, random_state=2000) # (50:50 VAL:TEST SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hj5fpBtW0s9r"
   },
   "outputs": [],
   "source": [
    "make_tagged_docs(clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBoTKkODKj1B"
   },
   "outputs": [],
   "source": [
    "train_dbow_d2v(model_name_dbow, 100) [1.6 Million tweets, 100 Dimensions]\n",
    "train_dmm_d2v(model_name_dmm, 100) [1.6 Million tweets, 100 Dimensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsr4QGEUKj1F"
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load(model_name_dbow) # This is my base-line DBOW model\n",
    "print('LOADED DBOW MODEL [1.6 Million tweets, 100 Dimensions]') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xm9fADVFnCdw"
   },
   "outputs": [],
   "source": [
    "model_dmm = Doc2Vec.load(model_name_dmm) # This is my base-line DMM Model\n",
    "print('LOADED DMM MODEL [1.6 Million tweets, 100 Dimensions]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aRRNXuKP-AIw"
   },
   "outputs": [],
   "source": [
    "lr_dbow_100, pos_avg_dbow_100, neg_avg_dbow_100, lda_dbow_100 = model_eval(model_dbow, 100)\n",
    "# LR Accuracy: 0.7413533834586467 for DBOW\n",
    "# CS Accuracy: 0.7291979949874686 for DBOW\n",
    "# LDA Accuracy: 0.7411654135338346 for DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOVV7LhlDXyA"
   },
   "outputs": [],
   "source": [
    "lr_dmm_100, pos_avg_dmm_100, neg_avg_dmm_100, lda_dmm_100 = model_eval(model_dmm, 100)\n",
    "# LR Accuracy: 0.7288220551378446 for DMM\n",
    "# CS Accuracy: 0.7229949874686716 for DMM\n",
    "# LDA Accuracy: 0.7280075187969924 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqIuuZRlKptV"
   },
   "outputs": [],
   "source": [
    "lr_combo_200, pos_avg_combo_200, neg_avg_combo_200, lda_combo_200 = model_eval(model_dbow, 200, model_dmm)\n",
    "# LR Accuracy: 0.755764411027569 for DBOW + DMM Combined\n",
    "# CS Accuracy: 0.7382832080200501 for DBOW + DMM Combined\n",
    "# LDA Accuracy: 0.7541353383458647 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D86AIBpeBTV1"
   },
   "outputs": [],
   "source": [
    "tweet = 'what a beautiful day'\n",
    "get_sentiment(tweet, model_dbow, 200, lr_combo_200, pos_avg_combo_200, neg_avg_combo_200, lda_combo_200, model_dmm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying the sizes of the dimensions for Doc2Vec Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fMU08X8kV_pa"
   },
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_dbow_200dim.model'\n",
    "model_name_dmm = 'doc2vecmodel_dmm_200dim.model'\n",
    "# train_dbow_d2v(model_name_dbow, 200)\n",
    "# train_dmm_d2v(model_name_dmm, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdoFDRrFWWAm"
   },
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_dbow_75dim.model'\n",
    "model_name_dmm = 'doc2vecmodel_dmm_75dim.model'\n",
    "# train_dbow_d2v(model_name_dbow, 75)\n",
    "# train_dmm_d2v(model_name_dmm, 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t_h_GRePWjkS"
   },
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_dbow_50dim.model'\n",
    "model_name_dmm = 'doc2vecmodel_dmm_50dim.model'\n",
    "# train_dbow_d2v(model_name_dbow, 50)\n",
    "# train_dmm_d2v(model_name_dmm, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bZuHrA8cWmUh"
   },
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_dbow_25dim.model'\n",
    "model_name_dmm = 'doc2vecmodel_dmm_25dim.model'\n",
    "# train_dbow_d2v(model_name_dbow, 25)\n",
    "# train_dmm_d2v(model_name_dmm, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yMcDwQhpWoyM"
   },
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_dbow_10dim.model'\n",
    "model_name_dmm = 'doc2vecmodel_dmm_10dim.model'\n",
    "# train_dbow_d2v(model_name_dbow, 10)\n",
    "# train_dmm_d2v(model_name_dmm, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load('doc2vecmodel_dbow_200dim.model')\n",
    "print('LOADED DBOW (200 DIM) MODEL\\n')\n",
    "model_dmm = Doc2Vec.load('doc2vecmodel_dmm_200dim.model')\n",
    "print('LOADED DMM (200 DIM) MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dbow_200, pos_avg_dbow_200, neg_avg_dbow_200, lda_dbow_200 = model_eval(model_dbow, 200)\n",
    "# LR Accuracy: 0.7471804511278195 for DBOW\n",
    "# CS Accuracy: 0.7302631578947368 for DBOW\n",
    "# LDA Accuracy: 0.7478696741854637 for DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dmm_200, pos_avg_dmm_200, neg_avg_dmm_200, lda_dmm_200 = model_eval(model_dmm, 200)\n",
    "# LR Accuracy: 0.7309523809523809 for DMM\n",
    "# CS Accuracy: 0.7226817042606516 for DMM\n",
    "# LDA Accuracy: 0.7293859649122807 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_combo_400, pos_avg_combo_400, neg_avg_combo_400, lda_combo_400 = model_eval(model_dbow, 400, model_dmm)\n",
    "# LR Accuracy: 0.755764411027569 for DBOW + DMM Combined\n",
    "# CS Accuracy: 0.7382832080200501 for DBOW + DMM Combined\n",
    "# LDA Accuracy: [MEM ERROR] for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load('doc2vecmodel_dbow_75dim.model')\n",
    "print('LOADED DBOW (75 DIM) MODEL\\n')\n",
    "model_dmm = Doc2Vec.load('doc2vecmodel_dmm_75dim.model')\n",
    "print('LOADED DMM (75 DIM) MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dbow_75, pos_avg_dbow_75, neg_avg_dbow_75, lda_dbow_75 = model_eval(model_dbow, 75)\n",
    "# LR Accuracy: 0.7390977443609023 for DBOW\n",
    "# CS Accuracy: 0.7293233082706767 for DBOW\n",
    "# LDA Accuracy: 0.7391604010025062 for DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dmm_75, pos_avg_dmm_75, neg_avg_dmm_75, lda_dmm_75 = model_eval(model_dmm, 75)\n",
    "# LR Accuracy: 0.7294486215538847 for DMM\n",
    "# CS Accuracy: 0.7206140350877193 for DMM\n",
    "# LDA Accuracy: 0.7285714285714285 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_combo_150, pos_avg_combo_150, neg_avg_combo_150, lda_combo_150 = model_eval(model_dbow, 150, model_dmm)\n",
    "# LR Accuracy: 0.750438596491228 for DBOW + DMM Combined\n",
    "# CS Accuracy: 0.7380952380952381 for DBOW + DMM Combined\n",
    "# LDA Accuracy: 0.750062656641604 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load('doc2vecmodel_dbow_50dim.model')\n",
    "print('LOADED DBOW (50 DIM) MODEL\\n')\n",
    "model_dmm = Doc2Vec.load('doc2vecmodel_dmm_50dim.model')\n",
    "print('LOADED DMM (50 DIM) MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dbow_50, pos_avg_dbow_50, neg_avg_dbow_50, lda_dbow_50 = model_eval(model_dbow, 50)\n",
    "# LR Accuracy: 0.7328947368421053 for DMM\n",
    "# CS Accuracy: 0.7271929824561404 for DMM\n",
    "# LDA Accuracy: 0.7326441102756892 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dmm_50, pos_avg_dmm_50, neg_avg_dmm_50, lda_dmm_50 = model_eval(model_dmm, 50)\n",
    "# LR Accuracy: 0.7240601503759398 for DMM\n",
    "# CS Accuracy: 0.718859649122807 for DMM\n",
    "# LDA Accuracy: 0.7228696741854637 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_combo_100, pos_avg_combo_100, neg_avg_combo_100, lda_combo_100 = model_eval(model_dbow, 100, model_dmm)\n",
    "# LR Accuracy: 0.7432957393483709 for DBOW + DMM Combined\n",
    "# CS Accuracy: 0.7293859649122807 for DBOW + DMM Combined\n",
    "# LDA Accuracy: 0.7422932330827068 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load('doc2vecmodel_dbow_25dim.model')\n",
    "print('LOADED DBOW (25 DIM) MODEL\\n')\n",
    "model_dmm = Doc2Vec.load('doc2vecmodel_dmm_25dim.model')\n",
    "print('LOADED DMM (25 DIM) MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dbow_25, pos_avg_dbow_25, neg_avg_dbow_25, lda_dbow_25 = model_eval(model_dbow, 25)\n",
    "# LR Accuracy: 0.7305764411027569 for DBOW\n",
    "# CS Accuracy: 0.7268170426065163 for DBOW\n",
    "# LDA Accuracy: 0.7308270676691729 for DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dmm_25, pos_avg_dmm_25, neg_avg_dmm_25, lda_dmm_25 = model_eval(model_dmm, 25)\n",
    "# LR Accuracy: 0.712593984962406 for DMM\n",
    "# CS Accuracy: 0.7100250626566416 for DMM\n",
    "# LDA Accuracy: 0.712218045112782 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_combo_50, pos_avg_combo_50, neg_avg_combo_50, lda_combo_50 = model_eval(model_dbow, 50, model_dmm)\n",
    "# LR Accuracy: 0.7328947368421053 for DBOW + DMM Combined\n",
    "# CS Accuracy: 0.7236842105263158 for DBOW + DMM Combined\n",
    "# LDA Accuracy: 0.7334586466165414 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load('doc2vecmodel_dbow_10dim.model')\n",
    "print('LOADED DBOW (10 DIM) MODEL\\n')\n",
    "model_dmm = Doc2Vec.load('doc2vecmodel_dmm_10dim.model')\n",
    "print('LOADED DMM (10 DIM) MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dbow_10, pos_avg_dbow_10, neg_avg_dbow_10, lda_dbow_10 = model_eval(model_dbow, 10)\n",
    "# LR Accuracy: 0.7095864661654135 for DBOW\n",
    "# CS Accuracy: 0.706390977443609 for DBOW\n",
    "# LDA Accuracy: 0.7093358395989975 for DBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_dmm_10, pos_avg_dmm_10, neg_avg_dmm_10, lda_dmm_10 = model_eval(model_dmm, 10)\n",
    "# LR Accuracy: 0.6843358395989975 for DMM\n",
    "# CS Accuracy: 0.680576441102757 for DMM\n",
    "# LDA Accuracy: 0.6849624060150376 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_combo_20, pos_avg_combo_20, neg_avg_combo_20, lda_combo_20 = model_eval(model_dbow, 20, model_dmm)\n",
    "# LR Accuracy: 0.712155388471178 for DBOW + DMM Combined\n",
    "# CS Accuracy: 0.6941729323308271 for DBOW + DMM Combined\n",
    "# LDA Accuracy: 0.7112781954887218 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Word Vectors from Doc2Vec Model and using Average Word Vector (100-DIM DBOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec, val_vec, test_vec = extract_wordvec(model_dbow, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.618859649122807 for DBOW\n",
    "# Accuracy: 0.7165413533834587 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec, y_train, 100)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec, y_validation, 100)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.6110275689223058 for DBOW\n",
    "# Accuracy: 0.6630325814536341 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.6298872180451128 for DBOW\n",
    "# Accuracy: 0.7156641604010026 for DMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oDJtFEC-CiXp"
   },
   "outputs": [],
   "source": [
    "keras.backend.backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron (Combo Doc Vectors, 256 nodes, 3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_combo = model_combine(model_dbow, model_dmm, x_train, 200)\n",
    "val_vec_combo = model_combine(model_dbow, model_dmm, x_validation, 200)\n",
    "test_vec_combo = model_combine(model_dbow, model_dmm, x_test, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"combo_model_multi_layer_perceptron_256_3.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EovmeU_JwFdt"
   },
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_combo_256_3 = Sequential()\n",
    "nn_model_combo_256_3.add(Dense(256, activation='relu', input_dim=200))\n",
    "nn_model_combo_256_3.add(Dense(256, activation='relu'))\n",
    "nn_model_combo_256_3.add(Dense(256, activation='relu'))\n",
    "nn_model_combo_256_3.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_combo_256_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_combo_256_3.fit(train_vec_combo, y_train, validation_data=(val_vec_combo, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_combo_256_3 = load_model('combo_model_multi_layer_perceptron_256_3.hd5')\n",
    "nn_model_combo_256_3.evaluate(x=val_vec_combo, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.79605 with es 5 and 50 epochs (val acc = 0.7960526347160339, and val loss = 0.4394365343085507) at epoch 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron (DBOW Doc Vectors, 256 nodes, 3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dbow, x_train, 100)\n",
    "val_vec = model_to_vec(model_dbow, x_validation, 100)\n",
    "test_vec = model_to_vec(model_dbow, x_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"dbow_model_multi_layer_perceptron_256_3.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_dbow_256_3 = Sequential()\n",
    "nn_model_dbow_256_3.add(Dense(256, activation='relu', input_dim=100))\n",
    "nn_model_dbow_256_3.add(Dense(256, activation='relu'))\n",
    "nn_model_dbow_256_3.add(Dense(256, activation='relu'))\n",
    "nn_model_dbow_256_3.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_dbow_256_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_dbow_256_3.fit(train_vec, y_train, validation_data=(val_vec, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_dbow_256_3 = load_model('dbow_model_multi_layer_perceptron_256_3.hd5')\n",
    "nn_model_dbow_256_3.evaluate(x=val_vec, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.78659 with es 5 and 50 epochs (val acc = 0.7865914702415466, and val loss = [0.45259528155613665) at epoch 7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron (DMM Doc Vectors, 256 nodes, 3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dmm, x_train, 100)\n",
    "val_vec = model_to_vec(model_dmm, x_validation, 100)\n",
    "test_vec = model_to_vec(model_dmm, x_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"dmm_model_multi_layer_perceptron_256_3.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_dmm_256_3 = Sequential()\n",
    "nn_model_dmm_256_3.add(Dense(256, activation='relu', input_dim=100))\n",
    "nn_model_dmm_256_3.add(Dense(256, activation='relu'))\n",
    "nn_model_dmm_256_3.add(Dense(256, activation='relu'))\n",
    "nn_model_dmm_256_3.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_dmm_256_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_dmm_256_3.fit(train_vec, y_train, validation_data=(val_vec, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_dmm_256_3 = load_model('dmm_model_multi_layer_perceptron_256_3.hd5')\n",
    "nn_model_dmm_256_3.evaluate(x=val_vec, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.76510 with es 5 and 50 epochs (val acc = 0.7651002407073975, and val loss = 0.4874301516770719) at epoch 9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron (Comb Doc Vectors, 256 nodes, 1 layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_combo = model_combine(model_dbow, model_dmm, x_train, 200)\n",
    "val_vec_combo = model_combine(model_dbow, model_dmm, x_validation, 200)\n",
    "test_vec_combo = model_combine(model_dbow, model_dmm, x_test, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"combo_model_multi_layer_perceptron_256_1.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_combo_256_1 = Sequential()\n",
    "nn_model_combo_256_1.add(Dense(256, activation='relu', input_dim=200))\n",
    "nn_model_combo_256_1.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_combo_256_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_combo_256_1.fit(train_vec_combo, y_train, validation_data=(val_vec_combo, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_combo_256_1 = load_model('combo_model_multi_layer_perceptron_256_1.hd5')\n",
    "nn_model_combo_256_1.evaluate(x=val_vec_combo, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.78465 with es 5 and 50 epochs (val acc = 0.784649133682251, and val loss = 0.45730126285015193) at epoch 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron (DBOW Doc Vectors, 256 nodes, 1 layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dbow, x_train, 100)\n",
    "val_vec = model_to_vec(model_dbow, x_validation, 100)\n",
    "test_vec = model_to_vec(model_dbow, x_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"dbow_model_multi_layer_perceptron_256_1.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_dbow_256_1 = Sequential()\n",
    "nn_model_dbow_256_1.add(Dense(256, activation='relu', input_dim=100))\n",
    "nn_model_dbow_256_1.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_dbow_256_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_dbow_256_1.fit(train_vec, y_train, validation_data=(val_vec, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_dbow_256_1 = load_model('dbow_model_multi_layer_perceptron_256_1.hd5')\n",
    "nn_model_dbow_256_1.evaluate(x=val_vec, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.77406 with es 5 and 50 epochs (val acc = 0.7740601301193237, and val loss = 0.4699582203289022) at epoch 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron (DMM Doc Vectors, 256 nodes, 1 layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dmm, x_train, 100)\n",
    "val_vec = model_to_vec(model_dmm, x_validation, 100)\n",
    "test_vec = model_to_vec(model_dmm, x_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"dmm_model_multi_layer_perceptron_256_1.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_dmm_256_1 = Sequential()\n",
    "nn_model_dmm_256_1.add(Dense(256, activation='relu', input_dim=100))\n",
    "nn_model_dmm_256_1.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_dmm_256_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_dmm_256_1.fit(train_vec, y_train, validation_data=(val_vec, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_dmm_256_1 = load_model('dmm_model_multi_layer_perceptron_256_1.hd5')\n",
    "nn_model_dmm_256_1.evaluate(x=val_vec, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.75150 with es 5 and 50 epochs (val acc = 0.7515037655830383, and val loss = 0.5119869738593137) at epoch 12\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron (Combo Doc Vectors, 128 nodes, 3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_combo = model_combine(model_dbow, model_dmm, x_train, 200)\n",
    "val_vec_combo = model_combine(model_dbow, model_dmm, x_validation, 200)\n",
    "test_vec_combo = model_combine(model_dbow, model_dmm, x_test, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"combo_model_multi_layer_perceptron_128_3.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_combo_128_3 = Sequential()\n",
    "nn_model_combo_128_3.add(Dense(128, activation='relu', input_dim=200))\n",
    "nn_model_combo_128_3.add(Dense(128, activation='relu'))\n",
    "nn_model_combo_128_3.add(Dense(128, activation='relu'))\n",
    "nn_model_combo_128_3.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_combo_128_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_combo_128_3.fit(train_vec_combo, y_train, validation_data=(val_vec_combo, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_combo_128_3 = load_model('combo_model_multi_layer_perceptron_128_3.hd5')\n",
    "nn_model_combo_128_3.evaluate(x=val_vec_combo, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.79192 with es 5 and 50 epochs (val acc = 0.7919172644615173, and val loss = 0.44302879301527687) at epoch 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron (DBOW Doc Vectors, 128 nodes, 3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dbow, x_train, 100)\n",
    "val_vec = model_to_vec(model_dbow, x_validation, 100)\n",
    "test_vec = model_to_vec(model_dbow, x_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"dbow_model_multi_layer_perceptron_128_3.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_dbow_128_3 = Sequential()\n",
    "nn_model_dbow_128_3.add(Dense(128, activation='relu', input_dim=100))\n",
    "nn_model_dbow_128_3.add(Dense(128, activation='relu'))\n",
    "nn_model_dbow_128_3.add(Dense(128, activation='relu'))\n",
    "nn_model_dbow_128_3.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_dbow_128_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_dbow_128_3.fit(train_vec, y_train, validation_data=(val_vec, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_dbow_128_3 = load_model('dbow_model_multi_layer_perceptron_128_3.hd5')\n",
    "nn_model_dbow_128_3.evaluate(x=val_vec, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.78289 with es 5 and 50 epochs (val acc = 0.7828947305679321, and val loss = 0.45956595229325736) at epoch 18\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-layer Perceptron (DMM Doc Vectors, 128 nodes, 3 layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dmm, x_train, 100)\n",
    "val_vec = model_to_vec(model_dmm, x_validation, 100)\n",
    "test_vec = model_to_vec(model_dmm, x_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"dmm_model_multi_layer_perceptron_128_3.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_dmm_128_3 = Sequential()\n",
    "nn_model_dmm_128_3.add(Dense(128, activation='relu', input_dim=100))\n",
    "nn_model_dmm_128_3.add(Dense(128, activation='relu'))\n",
    "nn_model_dmm_128_3.add(Dense(128, activation='relu'))\n",
    "nn_model_dmm_128_3.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_dmm_128_3.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_dmm_128_3.fit(train_vec, y_train, validation_data=(val_vec, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_dmm_128_3 = load_model('dmm_model_multi_layer_perceptron_128_3.hd5')\n",
    "nn_model_dmm_128_3.evaluate(x=val_vec, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.76190 with es 5 and 50 epochs (val acc = 0.761904776096344, and val loss = 0.49163153473297155) at epoch 14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Multi-layer Perceptron (Combo Doc Vectors, 128 nodes, 1 layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec_combo = model_combine(model_dbow, model_dmm, x_train, 200)\n",
    "val_vec_combo = model_combine(model_dbow, model_dmm, x_validation, 200)\n",
    "test_vec_combo = model_combine(model_dbow, model_dmm, x_test, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"combo_model_multi_layer_perceptron_128_1.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_combo_128_1 = Sequential()\n",
    "nn_model_combo_128_1.add(Dense(128, activation='relu', input_dim=200))\n",
    "nn_model_combo_128_1.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_combo_128_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_combo_128_1.fit(train_vec_combo, y_train, validation_data=(val_vec_combo, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_combo_128_1 = load_model('combo_model_multi_layer_perceptron_128_1.hd5')\n",
    "nn_model_combo_128_1.evaluate(x=val_vec_combo, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.78296 with es 5 and 50 epochs (val acc = 0.7829573750495911, and val loss = 0.4597198429711182) at epoch 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Multi-layer Perceptron (DBOW Doc Vectors, 128 nodes, 1 layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dbow, x_train, 100)\n",
    "val_vec = model_to_vec(model_dbow, x_validation, 100)\n",
    "test_vec = model_to_vec(model_dbow, x_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"dbow_model_multi_layer_perceptron_128_1.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_dbow_128_1 = Sequential()\n",
    "nn_model_dbow_128_1.add(Dense(128, activation='relu', input_dim=100))\n",
    "nn_model_dbow_128_1.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_dbow_128_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_dbow_128_1.fit(train_vec, y_train, validation_data=(val_vec, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_dbow_128_1 = load_model('dbow_model_multi_layer_perceptron_128_1.hd5')\n",
    "nn_model_dbow_128_1.evaluate(x=val_vec, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.77368 with es 5 and 50 epochs (val acc = 0.7736842036247253, and val loss = 0.47306825482755677) at epoch 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Multi-layer Perceptron (DMM Doc Vectors, 128 nodes, 1 layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dmm, x_train, 100)\n",
    "val_vec = model_to_vec(model_dmm, x_validation, 100)\n",
    "test_vec = model_to_vec(model_dmm, x_test, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = \"dmm_model_multi_layer_perceptron_128_1.hd5\"\n",
    "checkpoint = ModelCheckpoint(model_file_name, monitor='val_accuracy', verbose=1, save_best_only=True, mode='max')\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', patience=5, mode='max') \n",
    "callback_arr = [checkpoint, early_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2000)\n",
    "nn_model_dmm_128_1 = Sequential()\n",
    "nn_model_dmm_128_1.add(Dense(128, activation='relu', input_dim=100))\n",
    "nn_model_dmm_128_1.add(Dense(1, activation='sigmoid'))\n",
    "nn_model_dmm_128_1.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "nn_model_dmm_128_1.fit(train_vec, y_train, validation_data=(val_vec, y_validation), epochs=50, batch_size=32, verbose=1, callbacks=callback_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_model_dmm_128_1 = load_model('dmm_model_multi_layer_perceptron_128_1.hd5')\n",
    "nn_model_dmm_128_1.evaluate(x=val_vec, y=y_validation)\n",
    "# VALIDATION ACCURACY: 0.75144 with es 5 and 50 epochs (val acc = 0.7514411211013794, and val loss = 0.5119231587812715) at epoch 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CO35B38MKj1u"
   },
   "outputs": [],
   "source": [
    "# create_tweet_samples(clean_df, 1000)\n",
    "# create_tweet_samples(clean_df, 10000)\n",
    "# create_tweet_samples(clean_df, 100000)\n",
    "# create_tweet_samples(clean_df, 250000)\n",
    "# create_tweet_samples(clean_df, 500000)\n",
    "# create_tweet_samples(clean_df, 1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pZocEocJKj1x"
   },
   "source": [
    "## Experiment 1 (1000 Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nH2CfKPYKj1z"
   },
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_dbow_1000.model'\n",
    "model_name_dmm = 'doc2vecmodel_dmm_1000.model'\n",
    "df_1000 = pd.read_csv(\"tweets_sample_1000.csv\", encoding='latin-1', names=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fRrt3OHSKj11"
   },
   "outputs": [],
   "source": [
    "load_tweets(df_1000)\n",
    "x = df_1000.tweet\n",
    "y = df_1000.tag\n",
    "x_train, x_remain, y_train, y_remain = train_test_split(x, y, test_size=.02, random_state=2000)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_remain, y_remain, test_size=.5, random_state=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XwNw4RMwKj13"
   },
   "outputs": [],
   "source": [
    "make_tagged_docs(df_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CP_UVDr9Kj16"
   },
   "outputs": [],
   "source": [
    "# train_dbow_d2v(model_name_dbow, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NBUd_tJZKj17"
   },
   "outputs": [],
   "source": [
    "# train_dmm_d2v(model_name_dmm, 100) --REMEMBER TO CHANGE FOR ALL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x0if5SW-Kj1-"
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load(model_name_dbow)\n",
    "print('LOADED DBOW (1000) MODEL\\n')\n",
    "model_dmm = Doc2Vec.load(model_name_dmm)\n",
    "print('LOADED DMM (1000) MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xKUzlvp5Kj2A"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dbow, x_train)\n",
    "val_vec = model_to_vec(model_dbow, x_validation)\n",
    "test_vec = model_to_vec(model_dbow, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e-VsKv5TKj2B"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dmm, x_train)\n",
    "val_vec = model_to_vec(model_dmm, x_validation)\n",
    "test_vec = model_to_vec(model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jGrS_qsEKj2E"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.6 for DBOW\n",
    "# Accuracy: 0.5 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dSeqYA-iKj2G"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec, y_train, 100)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec, y_validation, 100)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.4 for DBOW\n",
    "# Accuracy: 0.6 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbkv7oQsKj2J"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.7 for DBOW\n",
    "# Accuracy: 0.4 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeaLP8-TTafY"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(train_vec, y_train)\n",
    "print('SVM Score: ')\n",
    "print(svm.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.6 for DBOW\n",
    "# Accuracy: 0.4 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PXJd1xt6Kj2N"
   },
   "outputs": [],
   "source": [
    "train_vec_combo = model_combine(model_dbow, model_dmm, x_train)\n",
    "val_vec_combo = model_combine(model_dbow, model_dmm, x_validation)\n",
    "test_vec_combo = model_combine(model_dbow, model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqfVDLvXKj2P"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec_combo, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A47-x3B5Kj2U"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec_combo, y_train, 200)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec_combo, y_validation, 200)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6LgbpVPxKj2V"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec_combo, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.4 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eVe6d904Xzvp"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "svm.fit(train_vec_combo, y_train)\n",
    "print('SVM Score: ')\n",
    "print(svm.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.6 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2lPLZdu7jqwc"
   },
   "outputs": [],
   "source": [
    "def obj(trial):\n",
    "    svc_c = trial.suggest_loguniform('C', 1e0, 1e2)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf'])\n",
    "    svm = SVC(C=svc_c, kernel=kernel)\n",
    "    svm.fit(train_vec_combo, y_train)\n",
    "    score = cross_val_score(svm, train_vec_combo, y_train, n_jobs=-1, cv=3)\n",
    "    accuracy = score.mean()\n",
    "    return 1.0 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OzG43mFk3LQ"
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d7AibRGgk3Ur"
   },
   "outputs": [],
   "source": [
    "study.optimize(obj, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IgA8Oxbsk3Rt"
   },
   "outputs": [],
   "source": [
    "svm = SVC(C=6.336527825194087, kernel='poly')\n",
    "svm.fit(train_vec_combo, y_train)\n",
    "print('SVM Score: ')\n",
    "print(svm.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.7 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kLf8CPsvKj2Y"
   },
   "source": [
    "## Experiment 2 (10000 Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujnMTCZNKj2Z"
   },
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_dbow_10000.model'\n",
    "model_name_dmm = 'doc2vecmodel_dmm_10000.model'\n",
    "df_10000 = pd.read_csv(\"tweets_sample_10000.csv\", encoding='latin-1', names=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A3uP_ow2Kj2a"
   },
   "outputs": [],
   "source": [
    "load_tweets(df_10000)\n",
    "x = df_10000.tweet\n",
    "y = df_10000.tag\n",
    "x_train, x_remain, y_train, y_remain = train_test_split(x, y, test_size=.02, random_state=2000)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_remain, y_remain, test_size=.5, random_state=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0yWY1-y-Kj2d"
   },
   "outputs": [],
   "source": [
    "# make_tagged_docs(df_10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gpd8w9MnKj2f"
   },
   "outputs": [],
   "source": [
    "# train_dbow_d2v(model_name_dbow, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S-LhWcjbKj2h"
   },
   "outputs": [],
   "source": [
    "# train_dmm_d2v(model_name_dmm, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wj4OypgDKj2j"
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load(model_name_dbow)\n",
    "print('LOADED DBOW (10000) MODEL\\n')\n",
    "model_dmm = Doc2Vec.load(model_name_dmm)\n",
    "print('LOADED DMM (10000) MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0NOUwBCpKj2l"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dbow, x_train)\n",
    "val_vec = model_to_vec(model_dbow, x_validation)\n",
    "test_vec = model_to_vec(model_dbow, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XIEk4JmjKj21"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dmm, x_train)\n",
    "val_vec = model_to_vec(model_dmm, x_validation)\n",
    "test_vec = model_to_vec(model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S2lQtP_yKj23"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.51 for DBOW\n",
    "# Accuracy: 0.53 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ME-3sZyjKj24"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec, y_train, 100)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec, y_validation, 100)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.52 for DBOW\n",
    "# Accuracy: 0.49 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tMcieNVrKj26"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5 for DBOW\n",
    "# Accuracy: 0.54 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k0X0cs1z-BrH"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='rbf')\n",
    "svm.fit(train_vec, y_train)\n",
    "print('SVM Score: ')\n",
    "print(svm.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.58 for DBOW\n",
    "# Accuracy: 0.55 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHV99DO-Kj28"
   },
   "outputs": [],
   "source": [
    "train_vec_combo = model_combine(model_dbow, model_dmm, x_train)\n",
    "val_vec_combo = model_combine(model_dbow, model_dmm, x_validation)\n",
    "test_vec_combo = model_combine(model_dbow, model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ga25w16cKj2-"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec_combo, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.46 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hhT2xA8-Kj3F"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec_combo, y_train, 200)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec_combo, y_validation, 200)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "buivNQxsKj3H"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec_combo, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.45 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K5fKrWUtYC2_"
   },
   "outputs": [],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "svm.fit(train_vec_combo, y_train)\n",
    "print('SVM Score: ')\n",
    "print(svm.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "51c_dbP4nQHj"
   },
   "outputs": [],
   "source": [
    "def obj(trial):\n",
    "    svc_c = trial.suggest_loguniform('C', 1e0, 1e2)\n",
    "    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf'])\n",
    "    svm = SVC(C=svc_c, kernel=kernel)\n",
    "    svm.fit(train_vec_combo, y_train)\n",
    "    score = cross_val_score(svm, train_vec_combo, y_train, n_jobs=-1, cv=3)\n",
    "    accuracy = score.mean()\n",
    "    return 1.0 - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "k6RJy-1XnXVK"
   },
   "outputs": [],
   "source": [
    "study = optuna.create_study()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "--LMGphGnYCr"
   },
   "outputs": [],
   "source": [
    "study.optimize(obj, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xtuDBHKeneG1"
   },
   "outputs": [],
   "source": [
    "svm = SVC(C=6.336527825194087, kernel='poly')\n",
    "svm.fit(train_vec_combo, y_train)\n",
    "print('SVM Score: ')\n",
    "print(svm.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.57 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qU3R9CcNKj3I"
   },
   "source": [
    "## Experiment 3 (100000 Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yhd2AT4gKj3I"
   },
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_dbow_100000.model'\n",
    "model_name_dmm = 'doc2vecmodel_dmm_100000.model'\n",
    "df_100000 = pd.read_csv(\"tweets_sample_100000.csv\", encoding='latin-1', names=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5PjvcJoqKj3L"
   },
   "outputs": [],
   "source": [
    "load_tweets(df_100000)\n",
    "x = df_100000.tweet\n",
    "y = df_100000.tag\n",
    "x_train, x_remain, y_train, y_remain = train_test_split(x, y, test_size=.02, random_state=2000)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_remain, y_remain, test_size=.5, random_state=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XqKLlC6bKj3M"
   },
   "outputs": [],
   "source": [
    "# make_tagged_docs(df_100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K__jYF8ZKj3O"
   },
   "outputs": [],
   "source": [
    "# train_dbow_d2v(model_name_dbow, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GaYWBrsEKj3P"
   },
   "outputs": [],
   "source": [
    "# train_dmm_d2v(model_name_dmm, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vdbqi5RqKj3R"
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load(model_name_dbow)\n",
    "print('LOADED DBOW (100000) MODEL\\n')\n",
    "model_dmm = Doc2Vec.load(model_name_dmm)\n",
    "print('LOADED DMM (100000) MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6AaX0-j_Kj3U"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dbow, x_train)\n",
    "val_vec = model_to_vec(model_dbow, x_validation)\n",
    "test_vec = model_to_vec(model_dbow, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxUvCI6jKj3V"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dmm, x_train)\n",
    "val_vec = model_to_vec(model_dmm, x_validation)\n",
    "test_vec = model_to_vec(model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3HhutJqJKj3W"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.521 for DBOW\n",
    "# Accuracy: 0.516 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPDzvonHKj3Y"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec, y_train, 100)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec, y_validation, 100)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.522 for DBOW\n",
    "# Accuracy: 0.512 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "prkKGmWxKj3b"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.52 for DBOW\n",
    "# Accuracy: 0.517 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0yRUFa9Kj3e"
   },
   "outputs": [],
   "source": [
    "train_vec_combo = model_combine(model_dbow, model_dmm, x_train)\n",
    "val_vec_combo = model_combine(model_dbow, model_dmm, x_validation)\n",
    "test_vec_combo = model_combine(model_dbow, model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0XNOpM1sKj3g"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec_combo, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.519 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_VlBK0gTKj3h"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec_combo, y_train, 200)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec_combo, y_validation, 200)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.513 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GoWHMLzJKj3j"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec_combo, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.521 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsTNvD5GKj3l"
   },
   "source": [
    "## Experiment 4 (250000 Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z4AZWFX3Kj3o"
   },
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_dbow_250000.model'\n",
    "model_name_dmm = 'doc2vecmodel_dmm_250000.model'\n",
    "df_250000 = pd.read_csv(\"tweets_sample_250000.csv\", encoding='latin-1', names=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "raU8bQ4QKj3r"
   },
   "outputs": [],
   "source": [
    "load_tweets(df_250000)\n",
    "x = df_250000.tweet\n",
    "y = df_250000.tag\n",
    "x_train, x_remain, y_train, y_remain = train_test_split(x, y, test_size=.02, random_state=2000)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_remain, y_remain, test_size=.5, random_state=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_t_TzY2IKj3s"
   },
   "outputs": [],
   "source": [
    "# make_tagged_docs(df_250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4H0-qFDBKj3u"
   },
   "outputs": [],
   "source": [
    "# train_dbow_d2v(model_name_dbow, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RgMKRWq1Kj3v"
   },
   "outputs": [],
   "source": [
    "# train_dmm_d2v(model_name_dmm, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAcDT0mzKj3x"
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load(model_name_dbow)\n",
    "print('LOADED DBOW (250000) MODEL\\n')\n",
    "model_dmm = Doc2Vec.load(model_name_dmm)\n",
    "print('LOADED DMM (250000) MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AHEoiKmXKj3y"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dbow, x_train)\n",
    "val_vec = model_to_vec(model_dbow, x_validation)\n",
    "test_vec = model_to_vec(model_dbow, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A_iHehKYKj33"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dmm, x_train)\n",
    "val_vec = model_to_vec(model_dmm, x_validation)\n",
    "test_vec = model_to_vec(model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jb1U-zFjKj34"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5408 for DBOW\n",
    "# Accuracy: 0.5216 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z9-Ca5M5Kj34"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec, y_train, 100)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec, y_validation, 100)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5396 for DBOW\n",
    "# Accuracy: 0.522 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j8sT1CCtKj37"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5412 for DBOW\n",
    "# Accuracy: 0.5212 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MfzmdpmdKj38"
   },
   "outputs": [],
   "source": [
    "train_vec_combo = model_combine(model_dbow, model_dmm, x_train)\n",
    "val_vec_combo = model_combine(model_dbow, model_dmm, x_validation)\n",
    "test_vec_combo = model_combine(model_dbow, model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KlhJTIoxKj39"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec_combo, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5432 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RvZH_GTFKj4A"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec_combo, y_train, 200)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec_combo, y_validation, 200)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5276 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kIrE1QF_Kj4D"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec_combo, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5432 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vygz3Y0kKj4E"
   },
   "source": [
    "## Experiment 5 (500000 Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGvct-GIKj4F"
   },
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_dbow_500000.model'\n",
    "model_name_dmm = 'doc2vecmodel_dmm_500000.model'\n",
    "df_500000 = pd.read_csv(\"tweets_sample_500000.csv\", encoding='latin-1', names=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iDfSTL5sKj4G"
   },
   "outputs": [],
   "source": [
    "load_tweets(df_500000)\n",
    "x = df_500000.tweet\n",
    "y = df_500000.tag\n",
    "x_train, x_remain, y_train, y_remain = train_test_split(x, y, test_size=.02, random_state=2000)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_remain, y_remain, test_size=.5, random_state=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M6L2rIhOKj4H"
   },
   "outputs": [],
   "source": [
    "# make_tagged_docs(df_500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6lwOGisfKj4J"
   },
   "outputs": [],
   "source": [
    "# train_dbow_d2v(model_name_dbow, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0G0OsP_7Kj4K"
   },
   "outputs": [],
   "source": [
    "# train_dmm_d2v(model_name_dmm, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kj7AtA32Kj4L"
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load(model_name_dbow)\n",
    "print('LOADED DBOW (500000) MODEL\\n')\n",
    "model_dmm = Doc2Vec.load(model_name_dmm)\n",
    "print('LOADED DMM (500000) MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FGeR5GyXKj4M"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dbow, x_train)\n",
    "val_vec = model_to_vec(model_dbow, x_validation)\n",
    "test_vec = model_to_vec(model_dbow, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ni82XYicKj4O"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dmm, x_train)\n",
    "val_vec = model_to_vec(model_dmm, x_validation)\n",
    "test_vec = model_to_vec(model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1dH9JC4hKj4P"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5184 for DBOW\n",
    "# Accuracy: 0.5088 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qPFnYdm2Kj4Q"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec, y_train, 100)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec, y_validation, 100)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5164 for DBOW\n",
    "# Accuracy: 0.514 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FN0-bM2wKj4R"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5182 for DBOW\n",
    "# Accuracy: 0.5088 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0P9Rh0EcKj4T"
   },
   "outputs": [],
   "source": [
    "train_vec_combo = model_combine(model_dbow, model_dmm, x_train)\n",
    "val_vec_combo = model_combine(model_dbow, model_dmm, x_validation)\n",
    "test_vec_combo = model_combine(model_dbow, model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0GOsE-VQKj4U"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec_combo, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.516 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s4_wyQQmKj4V"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec_combo, y_train, 200)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec_combo, y_validation, 200)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5116 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_pVbQtsFKj4X"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec_combo, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5156 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ETdhX1l3Kj4X"
   },
   "source": [
    "## Experiment 6 (1000000 Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sRpMeAfUKj4Y"
   },
   "outputs": [],
   "source": [
    "model_name_dbow = 'doc2vecmodel_dbow_1000000.model'\n",
    "model_name_dmm = 'doc2vecmodel_dmm_1000000.model'\n",
    "df_1000000 = pd.read_csv(\"tweets_sample_1000000.csv\", encoding='latin-1', names=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YfwWVZVHKj4Z"
   },
   "outputs": [],
   "source": [
    "load_tweets(df_1000000)\n",
    "x = df_1000000.tweet\n",
    "y = df_1000000.tag\n",
    "x_train, x_remain, y_train, y_remain = train_test_split(x, y, test_size=.02, random_state=2000)\n",
    "x_validation, x_test, y_validation, y_test = train_test_split(x_remain, y_remain, test_size=.5, random_state=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mw7J6lPqKj4a"
   },
   "outputs": [],
   "source": [
    "# make_tagged_docs(df_1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3rKKb_tKj4b"
   },
   "outputs": [],
   "source": [
    "# train_dbow_d2v(model_name_dbow, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cw_x3OVSKj4c"
   },
   "outputs": [],
   "source": [
    "# train_dmm_d2v(model_name_dmm, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "33cU2RQxKj4e"
   },
   "outputs": [],
   "source": [
    "model_dbow = Doc2Vec.load(model_name_dbow)\n",
    "print('LOADED DBOW (1000000) MODEL\\n')\n",
    "model_dmm = Doc2Vec.load(model_name_dmm)\n",
    "print('LOADED DMM (1000000) MODEL\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r3kBBqtpKj4e"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dbow, x_train)\n",
    "val_vec = model_to_vec(model_dbow, x_validation)\n",
    "test_vec = model_to_vec(model_dbow, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YOldSeE3Kj4g"
   },
   "outputs": [],
   "source": [
    "train_vec = model_to_vec(model_dmm, x_train)\n",
    "val_vec = model_to_vec(model_dmm, x_validation)\n",
    "test_vec = model_to_vec(model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anW-9MQlKj4h"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5796 for DBOW\n",
    "# Accuracy: 0.5779 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EhtBqFCCKj4j"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec, y_train, 100)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec, y_validation, 100)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5757 for DBOW\n",
    "# Accuracy: 0.5706 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L7fPUnq2Kj4k"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5799 for DBOW\n",
    "# Accuracy: 0.5777 for DMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xjtgA2iYKj4m"
   },
   "outputs": [],
   "source": [
    "train_vec_combo = model_combine(model_dbow, model_dmm, x_train)\n",
    "val_vec_combo = model_combine(model_dbow, model_dmm, x_validation)\n",
    "test_vec_combo = model_combine(model_dbow, model_dmm, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7uF-1hz3Kj4n"
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(train_vec_combo, y_train)\n",
    "print('Logistic Regression Score: ')\n",
    "print(lr.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5897 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AroLR8VNKj4p"
   },
   "outputs": [],
   "source": [
    "posavg, negavg = average_vectors(train_vec_combo, y_train, 200)\n",
    "cos_similarity = cosine_sim(posavg, negavg, val_vec_combo, y_validation, 200)\n",
    "print('Cosine Similarity Score: ')\n",
    "print(cos_similarity)\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.5809 for DBOW + DMM Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_jMo3d8WKj4q"
   },
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "lda.fit(train_vec_combo, y_train)\n",
    "print('Linear Discriminant Analysis Score: ')\n",
    "print(lda.score(val_vec_combo, y_validation))\n",
    "print('-----------------------------------')\n",
    "# Accuracy: 0.591 for DBOW + DMM Combined"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Disso.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
